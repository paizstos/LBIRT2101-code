#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Nov 24 14:51:02 2025

@author: adamdavidmalila
"""

# -*- coding: utf-8 -*-
"""
STEP 3 ‚Äî Contr√¥le qualit√© des r√©sultats et recommandations

Ce script :
  - lit metrics_step3.json (g√©n√©r√© par step3_evaluate_best.py)
  - lit y_train.npy / y_test.npy pour conna√Ætre la taille des datasets
  - applique quelques r√®gles simples :
      * test_accuracy < 0.70 ‚Üí performance jug√©e faible
      * 0.70 <= test_accuracy < 0.80 ‚Üí performance moyenne
      * test_accuracy >= 0.80 ‚Üí performance bonne
      * |accuracy_train - accuracy_test| > 0.15 ‚Üí suspicion d'overfitting
  - imprime un diagnostic en fran√ßais + recommandations :
      * ajouter des textes IA/humains
      * renforcer la r√©gularisation
      * √©ventuellement augmenter le vocabulaire ou retravailler le nettoyage texte
"""

from pathlib import Path
import json

import numpy as np

ROOT = Path(__file__).resolve().parents[1]
OUT_STEP2 = ROOT / "outputs_step2"
OUT_STEP3 = ROOT / "outputs_step3"


def main():
    print(f"üß≠ Racine projet : {ROOT}")
    print("=== STEP 3 ‚Äî Contr√¥le qualit√© ===")

    metrics_path = OUT_STEP3 / "metrics_step3.json"
    if not metrics_path.exists():
        raise FileNotFoundError(
            f"metrics_step3.json introuvable dans {OUT_STEP3}.\n"
            "Lance d'abord step3_evaluate_best.py."
        )

    metrics = json.loads(metrics_path.read_text(encoding="utf-8"))
    best_model_name = metrics.get("best_model_name", "UNKNOWN")
    train_scores = metrics.get("train_scores", {})
    test_scores = metrics.get("test_scores", {})

    acc_train = train_scores.get("accuracy", None)
    acc_test  = test_scores.get("accuracy", None)

    # Taille des datasets
    y_train = np.load(OUT_STEP2 / "y_train.npy", allow_pickle=True)
    y_test  = np.load(OUT_STEP2 / "y_test.npy", allow_pickle=True)

    n_train = len(y_train)
    n_test  = len(y_test)

    print(f"\nüì¶ Mod√®le √©valu√© : {best_model_name}")
    print(f"   - Taille TRAIN : {n_train}")
    print(f"   - Taille TEST  : {n_test}")

    print("\nüìä Rappel des scores :")
    print(f"   - accuracy_train : {acc_train:.3f}")
    print(f"   - accuracy_test  : {acc_test:.3f}")

    gap = abs(acc_train - acc_test)
    print(f"   - √©cart train/test : {gap:.3f}")

    # Diagnostic selon accuracy test
    print("\nü©∫ Diagnostic qualitatif :")
    if acc_test < 0.70:
        print("‚ùå Performance TEST faible (< 0.70).")
        print("   ‚Üí Le mod√®le distingue mal IA vs humain sur des donn√©es jamais vues.")
        print("   ‚Üí Recommandations :")
        print("      - augmenter le nombre de textes IA ET humains (ex : passer √† 150‚Äì200 par classe),")
        print("      - v√©rifier le nettoyage (en-t√™tes, boilerplate, doublons),")
        print("      - √©ventuellement tester d'autres repr√©sentations (n-grammes plus larges, min_df plus bas).")
    elif acc_test < 0.80:
        print("‚ö†Ô∏è Performance TEST moyenne (entre 0.70 et 0.80).")
        print("   ‚Üí Le mod√®le capte une partie des patterns, mais les fronti√®res sont encore floues.")
        print("   ‚Üí Recommandations :")
        print("      - si possible, ajouter quelques dizaines de textes IA/humains pour enrichir le signal,")
        print("      - affiner l'architecture (C du SVM, r√©gularisation de la logistic, etc.),")
        print("      - v√©rifier que les textes IA ne sont pas trop ‚Äòproches‚Äô des textes humains (m√™me style, m√™me longueur).")
    else:
        print("‚úÖ Performance TEST bonne (‚â• 0.80).")
        print("   ‚Üí Le mod√®le s√©pare correctement IA vs humain sur ce dataset.")
        print("   ‚Üí Tu peux consid√©rer ce niveau comme satisfaisant pour un projet acad√©mique.")
        print("   ‚Üí Tu peux maintenant te concentrer sur l'interpr√©tation et les visualisations (UMAP, cooccurrence, etc.).")

    # Diagnostic sur l‚Äôoverfitting
    print("\nüîé Analyse du risque d'overfitting :")
    if gap > 0.15:
        print("‚ö†Ô∏è Gros √©cart entre TRAIN et TEST (> 0.15) ‚Üí suspicion d'overfitting.")
        print("   ‚Üí Le mod√®le apprend trop les sp√©cificit√©s du TRAIN et g√©n√©ralise mal.")
        print("   ‚Üí Recommandations :")
        print("      - ajouter davantage de donn√©es (surtout dans la classe la moins vari√©e),")
        print("      - renforcer la r√©gularisation (ex : C plus petit pour SVM/logreg),")
        print("      - v√©rifier qu'il n'y a pas de textes quasi identiques entre train/test.")
    else:
        print("‚úÖ Pas d‚Äôoverfitting massif apparent (√©cart train/test raisonnable).")

    print("\nüìå R√©sum√© :")
    print("   - Ce script ne modifie pas les donn√©es, il t‚Äôaide √† d√©cider si le dataset est suffisant.")
    print("   - Si la performance est jug√©e limite, privil√©gie l‚Äôajout de nouveaux abstracts IA/humains,")
    print("     en gardant la sym√©trie entre les deux classes (m√™me ordre de grandeur).")

    print("\n‚úÖ Contr√¥le qualit√© termin√©.")


if __name__ == "__main__":
    main()